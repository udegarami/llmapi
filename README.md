# llmapi
a simple FastAPI to send voice prompts to a local LLM or the OpenAI API
Voice-to-LLM Integration with FastAPI

This is a simple FastAPI application that accepts voice prompts, transcribes them using OpenAI's Whisper model, and sends the transcribed text to a local LLM (GPT4All) or the OpenAI API for a response.

Features

Upload an audio file containing a voice prompt.
Audio is transcribed to text using Whisper.

Transcribed text is processed by either:
GPT4All: A framework enabling the use of locally hosted models.
OpenAI API: Uses OpenAI's GPT model for response generation.

## Requirements

The following Python packages are required to run this application:

fastapi: Framework for building the API.

uvicorn: ASGI server to run the FastAPI app.

openai-whisper: OpenAI's Whisper model for speech-to-text transcription.

gpt4all: Local GPT model for processing the transcribed text.

openai: OpenAI's API client to interact with their GPT model.

python-dotenv: To load environment variables for API keys.

### OS Version
The project was tested on a Virtualbox VM running Ubuntu 24.04 LTS, it should also work on Windows and MacOS with minor changes.

### Python Version

This project requires Python version 3.11.11.

## Installation

### Clone the repository:

    git clone https://github.com/your-username/llmapi.git
    cd llmapi

### Install dependencies:

    pip install -r requirements.txt

### Set up environment variables:

You will need to set up your OpenAI API key as an environment variable:

    export OPENAI_API_KEY="your-openai-api-key"

## Running the Application

To start the FastAPI app, use uvicorn:

    uvicorn main:app --reload

This will start the server at http://127.0.0.1:8000.
API Endpoints
POST /process-audio/

This endpoint processes an uploaded audio file and returns the transcription and the LLM response.

Request Parameters:

    file: (required) The audio file to be uploaded. Only .wav files are supported.
    model_choice: (optional) Choose between "GPT4All" and "OpenAI API". Defaults to "OpenAI API".

Response:

    transcription: The text transcribed from the audio.
    chatgpt_response: The response generated by the selected model (either GPT4All or OpenAI API).

Example request:

curl -X 'POST' \
  'http://127.0.0.1:8000/process-audio/?model_choice=OpenAI%20API' \
  -F 'file=@path_to_audio.wav'

Example response:

{
  "transcription": "Hello, how can I help you today?",
  "chatgpt_response": "Hi! How can I assist you today?"
}

## Error Handling

If any errors occur, the server will respond with an appropriate HTTP status code and message:

    500 Internal Server Error: General errors, such as issues with the Whisper transcription or OpenAI API connection.

## Clean Up

Temporary audio files are removed after processing to prevent storage buildup.

## License

This project is licensed under the MIT License. See the LICENSE file for more details.
